{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clone Github Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Harshal\\\\Data Science Projects\\\\Source-Code-Analysis-using-GenAI\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo 'c:\\\\Harshal\\\\Data Science Projects\\\\Source-Code-Analysis-using-GenAI\\\\research\\\\test_repo\\\\.git'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "Repo.clone_from(\"https://github.com/HARSHALKUMRE/Travel-Package-Purchase-Prediction-ML-Project\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "loader = GenericLoader.from_filesystem(repo_path+'/travel_pack',\n",
    "                                       glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser=LanguageParser(language=Language.PYTHON, parser_threshold=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='', metadata={'source': 'test_repo\\\\travel_pack\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import boto3\\nfrom travel_pack.configuration.aws_connection import S3Client\\nfrom io import StringIO\\nfrom typing import Union,List\\nimport os,sys\\nfrom travel_pack.logger import logging\\nfrom mypy_boto3_s3.service_resource import Bucket\\nfrom travel_pack.exception import TravelException\\nfrom botocore.exceptions import ClientError\\nfrom pandas import DataFrame,read_csv\\nimport pickle\\n\\n\\nclass SimpleStorageService:\\n\\n    def __init__(self):\\n        s3_client = S3Client()\\n        self.s3_resource = s3_client.s3_resource\\n        self.s3_client = s3_client.s3_client\\n\\n    def s3_key_path_available(self,bucket_name,s3_key)->bool:\\n        try:\\n            bucket = self.get_bucket(bucket_name)\\n            file_objects = [file_object for file_object in bucket.objects.filter(Prefix=s3_key)]\\n            if len(file_objects) > 0:\\n                return True\\n            else:\\n                return False\\n        except Exception as e:\\n            raise TravelException(e,sys)\\n        \\n        \\n\\n    @staticmethod\\n    def read_object(object_name: str, decode: bool = True, make_readable: bool = False) -> Union[StringIO, str]:\\n        \"\"\"\\n        Method Name :   read_object\\n        Description :   This method reads the object_name object with kwargs\\n\\n        Output      :   The column name is renamed\\n        On Failure  :   Write an exception log and then raise an exception\\n\\n        Version     :   1.2\\n        Revisions   :   moved setup to cloud\\n        \"\"\"\\n        logging.info(\"Entered the read_object method of S3Operations class\")\\n\\n        try:\\n            func = (\\n                lambda: object_name.get()[\"Body\"].read().decode()\\n                if decode is True\\n                else object_name.get()[\"Body\"].read()\\n            )\\n            conv_func = lambda: StringIO(func()) if make_readable is True else func()\\n            logging.info(\"Exited the read_object method of S3Operations class\")\\n            return conv_func()\\n\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n\\n    def get_bucket(self, bucket_name: str) -> Bucket:\\n        \"\"\"\\n        Method Name :   get_bucket\\n        Description :   This method gets the bucket object based on the bucket_name\\n\\n        Output      :   Bucket object is returned based on the bucket name\\n        On Failure  :   Write an exception log and then raise an exception\\n\\n        Version     :   1.2\\n        Revisions   :   moved setup to cloud\\n        \"\"\"\\n        logging.info(\"Entered the get_bucket method of S3Operations class\")\\n\\n        try:\\n            bucket = self.s3_resource.Bucket(bucket_name)\\n            logging.info(\"Exited the get_bucket method of S3Operations class\")\\n            return bucket\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n\\n    def get_file_object( self, filename: str, bucket_name: str) -> Union[List[object], object]:\\n        \"\"\"\\n        Method Name :   get_file_object\\n        Description :   This method gets the file object from bucket_name bucket based on filename\\n\\n        Output      :   list of objects or object is returned based on filename\\n        On Failure  :   Write an exception log and then raise an exception\\n\\n        Version     :   1.2\\n        Revisions   :   moved setup to cloud\\n        \"\"\"\\n        logging.info(\"Entered the get_file_object method of S3Operations class\")\\n\\n        try:\\n            bucket = self.get_bucket(bucket_name)\\n\\n            file_objects = [file_object for file_object in bucket.objects.filter(Prefix=filename)]\\n\\n            func = lambda x: x[0] if len(x) == 1 else x\\n\\n            file_objs = func(file_objects)\\n            logging.info(\"Exited the get_file_object method of S3Operations class\")\\n\\n            return file_objs\\n\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n\\n    def load_model(self, model_name: str, bucket_name: str, model_dir: str = None) -> object:\\n        \"\"\"\\n        Method Name :   load_model\\n        Description :   This method loads the model_name model from bucket_name bucket with kwargs\\n\\n        Output      :   list of objects or object is returned based on filename\\n        On Failure  :   Write an exception log and then raise an exception\\n\\n        Version     :   1.2\\n        Revisions   :   moved setup to cloud\\n        \"\"\"\\n        logging.info(\"Entered the load_model method of S3Operations class\")\\n\\n        try:\\n            func = (\\n                lambda: model_name\\n                if model_dir is None\\n                else model_dir + \"/\" + model_name\\n            )\\n            model_file = func()\\n            file_object = self.get_file_object(model_file, bucket_name)\\n            model_obj = self.read_object(file_object, decode=False)\\n            model = pickle.loads(model_obj)\\n            logging.info(\"Exited the load_model method of S3Operations class\")\\n            return model\\n\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n\\n    def create_folder(self, folder_name: str, bucket_name: str) -> None:\\n        \"\"\"\\n        Method Name :   create_folder\\n        Description :   This method creates a folder_name folder in bucket_name bucket\\n\\n        Output      :   Folder is created in s3 bucket\\n        On Failure  :   Write an exception log and then raise an exception\\n\\n        Version     :   1.2\\n        Revisions   :   moved setup to cloud\\n        \"\"\"\\n        logging.info(\"Entered the create_folder method of S3Operations class\")\\n\\n        try:\\n            self.s3_resource.Object(bucket_name, folder_name).load()\\n\\n        except ClientError as e:\\n            if e.response[\"Error\"][\"Code\"] == \"404\":\\n                folder_obj = folder_name + \"/\"\\n                self.s3_client.put_object(Bucket=bucket_name, Key=folder_obj)\\n            else:\\n                pass\\n            logging.info(\"Exited the create_folder method of S3Operations class\")\\n\\n    def upload_file(self, from_filename: str, to_filename: str,  bucket_name: str,  remove: bool = True):\\n        \"\"\"\\n        Method Name :   upload_file\\n        Description :   This method uploads the from_filename file to bucket_name bucket with to_filename as bucket filename\\n\\n        Output      :   Folder is created in s3 bucket\\n        On Failure  :   Write an exception log and then raise an exception\\n\\n        Version     :   1.2\\n        Revisions   :   moved setup to cloud\\n        \"\"\"\\n        logging.info(\"Entered the upload_file method of S3Operations class\")\\n\\n        try:\\n            logging.info(\\n                f\"Uploading {from_filename} file to {to_filename} file in {bucket_name} bucket\"\\n            )\\n\\n            self.s3_resource.meta.client.upload_file(\\n                from_filename, bucket_name, to_filename\\n            )\\n\\n            logging.info(\\n                f\"Uploaded {from_filename} file to {to_filename} file in {bucket_name} bucket\"\\n            )\\n\\n            if remove is True:\\n                os.remove(from_filename)\\n\\n                logging.info(f\"Remove is set to {remove}, deleted the file\")\\n\\n            else:\\n                logging.info(f\"Remove is set to {remove}, not deleted the file\")\\n\\n            logging.info(\"Exited the upload_file method of S3Operations class\")\\n\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n\\n    def upload_df_as_csv(self,data_frame: DataFrame,local_filename: str, bucket_filename: str,bucket_name: str,) -> None:\\n        \"\"\"\\n        Method Name :   upload_df_as_csv\\n        Description :   This method uploads the dataframe to bucket_filename csv file in bucket_name bucket\\n\\n        Output      :   Folder is created in s3 bucket\\n        On Failure  :   Write an exception log and then raise an exception\\n\\n        Version     :   1.2\\n        Revisions   :   moved setup to cloud\\n        \"\"\"\\n        logging.info(\"Entered the upload_df_as_csv method of S3Operations class\")\\n\\n        try:\\n            data_frame.to_csv(local_filename, index=None, header=True)\\n\\n            self.upload_file(local_filename, bucket_filename, bucket_name)\\n\\n            logging.info(\"Exited the upload_df_as_csv method of S3Operations class\")\\n\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n\\n    def get_df_from_object(self, object_: object) -> DataFrame:\\n        \"\"\"\\n        Method Name :   get_df_from_object\\n        Description :   This method gets the dataframe from the object_name object\\n\\n        Output      :   Folder is created in s3 bucket\\n        On Failure  :   Write an exception log and then raise an exception\\n\\n        Version     :   1.2\\n        Revisions   :   moved setup to cloud\\n        \"\"\"\\n        logging.info(\"Entered the get_df_from_object method of S3Operations class\")\\n\\n        try:\\n            content = self.read_object(object_, make_readable=True)\\n            df = read_csv(content, na_values=\"na\")\\n            logging.info(\"Exited the get_df_from_object method of S3Operations class\")\\n            return df\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n\\n    def read_csv(self, filename: str, bucket_name: str) -> DataFrame:\\n        \"\"\"\\n        Method Name :   get_df_from_object\\n        Description :   This method gets the dataframe from the object_name object\\n\\n        Output      :   Folder is created in s3 bucket\\n        On Failure  :   Write an exception log and then raise an exception\\n\\n        Version     :   1.2\\n        Revisions   :   moved setup to cloud\\n        \"\"\"\\n        logging.info(\"Entered the read_csv method of S3Operations class\")\\n\\n        try:\\n            csv_obj = self.get_file_object(filename, bucket_name)\\n            df = self.get_df_from_object(csv_obj)\\n            logging.info(\"Exited the read_csv method of S3Operations class\")\\n            return df\\n        except Exception as e:\\n            raise TravelException(e, sys) from e', metadata={'source': 'test_repo\\\\travel_pack\\\\cloud_storage\\\\aws_storage.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\travel_pack\\\\cloud_storage\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\n\\nfrom pandas import DataFrame\\nfrom sklearn.model_selection import train_test_split\\n\\nfrom travel_pack.exception import TravelException\\nfrom travel_pack.logger import logging\\nfrom travel_pack.entity.config_entity import DataIngestionConfig\\nfrom travel_pack.entity.artifact_entity import DataIngestionArtifact\\nfrom travel_pack.data_access.travel_data import TravelData\\n\\nclass DataIngestion:\\n    def __init__(self, data_ingestion_config: DataIngestionConfig=DataIngestionConfig()):\\n        \"\"\"\\n        :param data_ingestion_config: configuration for data ingestion\\n        \"\"\"\\n        try:\\n            self.data_ingestion_config = data_ingestion_config\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n        \\n    \\n    def export_data_into_feature_store(self) -> DataFrame:\\n        \"\"\"\\n        Method Name :   export_data_into_feature_store\\n        Description :   This method exports data from mongodb to csv file\\n        \\n        Output      :   data is returned as artifact of data ingestion components\\n        On Failure  :   Write an exception log and then raise an exception\\n        \"\"\"\\n        try:\\n            logging.info(f\"Exporting data from mongodb\")\\n            travel_db = TravelData()\\n            dataframe = travel_db.export_collection_as_dataframe(collection_name=self.data_ingestion_config.collection_name)\\n            logging.info(f\"Shape of dataframe: {dataframe.shape}\")\\n            feature_store_file_path = self.data_ingestion_config.feature_store_file_path\\n            dir_path = os.path.dirname(feature_store_file_path)\\n            os.makedirs(dir_path, exist_ok=True)\\n            logging.info(f\"Saving exported data into feature store file path: {feature_store_file_path}\")\\n            dataframe.to_csv(feature_store_file_path, index=False, header=True)\\n            \\n            return dataframe\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n        \\n    def split_data_as_train_test(self, dataframe: DataFrame) -> None:\\n        \"\"\"\\n        \"\"\"\\n        logging.info(\"Entered split_data_as_train_test method of Data_Ingestion class\")\\n\\n        try:\\n            train_set, test_set = train_test_split(dataframe, test_size=self.data_ingestion_config.train_test_split_ratio)\\n            logging.info(\"Performed train test split on the dataframe\")\\n            logging.info(\\n                \"Exited split_data_as_train_test method of Data_Ingestion class\"\\n            )\\n            dir_path = os.path.dirname(self.data_ingestion_config.training_file_path)\\n            os.makedirs(dir_path, exist_ok=True)\\n            \\n            logging.info(f\"Exporting train and test file path.\")\\n            train_set.to_csv(self.data_ingestion_config.training_file_path, index=False, header=True)\\n            test_set.to_csv(self.data_ingestion_config.testing_file_path, index=False, header=True)\\n            \\n            logging.info(f\"Exported train and test file path.\")\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n    \\n    def initiate_data_ingestion(self) ->DataIngestionArtifact:\\n        \"\"\"\\n        Method Name :   initiate_data_ingestion\\n        Description :   This method initiates the data ingestion components of training pipeline \\n        \\n        Output      :   train set and test set are returned as the artifacts of data ingestion components\\n        On Failure  :   Write an exception log and then raise an exception\\n        \"\"\"\\n        logging.info(\"Entered initiate_data_ingestion method of Data_Ingestion class\")\\n\\n        try:\\n            dataframe = self.export_data_into_feature_store()\\n\\n            logging.info(\"Got the data from mongodb\")\\n\\n            self.split_data_as_train_test(dataframe)\\n\\n            logging.info(\"Performed train test split on the dataset\")\\n\\n            logging.info(\\n                \"Exited initiate_data_ingestion method of Data_Ingestion class\"\\n            )\\n\\n            data_ingestion_artifact = DataIngestionArtifact(trained_file_path=self.data_ingestion_config.training_file_path,\\n            test_file_path=self.data_ingestion_config.testing_file_path)\\n            \\n            logging.info(f\"Data ingestion artifact: {data_ingestion_artifact}\")\\n            return data_ingestion_artifact\\n        except Exception as e:\\n            raise TravelException(e, sys) from e', metadata={'source': 'test_repo\\\\travel_pack\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom imblearn.combine import SMOTEENN\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\\n\\nfrom travel_pack.exception import TravelException\\nfrom travel_pack.logger import logging\\nfrom travel_pack.entity.config_entity import DataTransformationConfig\\nfrom travel_pack.entity.artifact_entity import DataIngestionArtifact, DataValidationArtifact, DataTransformationArtifact\\nfrom travel_pack.constants import TARGET_COLUMN, SCHEMA_FILE_PATH, RANDOM_STATE\\nfrom travel_pack.utils.main_utils import save_numpy_array_data, read_yaml_file, drop_columns, save_object\\n\\nclass DataTransformation:\\n    def __init__(self, data_ingestion_artifact: DataIngestionArtifact,\\n                 data_transformation_config: DataTransformationConfig,\\n                 data_validation_artifact: DataValidationArtifact):\\n        \"\"\"\\n        :param data_ingestion_artifact: Output reference of data ingestion artifact stage\\n        :param data_transformation_config: configuration for data transformation\\n        \"\"\"\\n        try:\\n            self.data_ingestion_artifact = data_ingestion_artifact\\n            self.data_transformation_config = data_transformation_config\\n            self.data_validation_artifact = data_validation_artifact\\n            self._schema_config = read_yaml_file(file_path=SCHEMA_FILE_PATH)\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n\\n    @staticmethod\\n    def read_data(file_path) -> pd.DataFrame:\\n        try:\\n            return pd.read_csv(file_path)\\n        except Exception as e:\\n            raise TravelException(e, sys)\\n        \\n        \\n    def get_data_transformer_object(self) -> Pipeline:\\n        \"\"\"\\n        Method Name :   get_data_transformer_object\\n        Description :   This method creates and returns a data transformer object for the data\\n        \\n        Output      :   data transformer object is created and returned \\n        On Failure  :   Write an exception log and then raise an exception\\n        \"\"\"\\n        logging.info(\\n            \"Entered get_data_transformer_object method of DataTransformation class\"\\n        )\\n        \\n        try:\\n            logging.info(\"Got numerical, categorical, transformation columns from schema config\")\\n            \\n            discrete_columns = self._schema_config[\\'discrete_columns\\']\\n            continuous_columns = self._schema_config[\\'continuous_columns\\']\\n            categorical_columns = self._schema_config[\\'categorical_columns\\']\\n            transformation_columns = self._schema_config[\\'transform_columns\\']\\n            \\n            logging.info(\\n                \"Got numerical cols,one hot cols,binary cols from schema config\"\\n            )\\n\\n            logging.info(\"Initialized Data Transformer pipeline.\")\\n            \\n            discrete_pipeline = Pipeline(\\n                steps=[\\n                    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\\n                    (\"scaler\", StandardScaler()),\\n                ]\\n            ) \\n            \\n            continuous_pipeline = Pipeline(\\n                steps=[\\n                    (\"imputer\", SimpleImputer(strategy=\"mean\")),\\n                    (\"scaler\", StandardScaler()),\\n                ]\\n            )\\n            \\n            cat_pipeline = Pipeline(\\n                steps=[\\n                    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\\n                    (\"one_hot_encoder\", OneHotEncoder()),\\n                    (\"scaler\", StandardScaler(with_mean=False)),\\n                ]\\n            )\\n            \\n            transform_pipeline = Pipeline(\\n                steps=[\\n                    (\"imputer\", SimpleImputer(strategy=\"mean\")),\\n                    (\"transformer\", PowerTransformer(standardize=True)),\\n                ]\\n            )\\n            \\n            preprocessor = ColumnTransformer(\\n                [\\n                    (\"discrete_pipeline\", discrete_pipeline, discrete_columns),\\n                    (\"continuous_pipeline\", continuous_pipeline, continuous_columns),\\n                    (\"cat_pipeline\", cat_pipeline, categorical_columns),\\n                    (\"power_transformation\", transform_pipeline, transformation_columns),\\n                ]\\n            )\\n            \\n            logging.info(\"Created preprocessor object from ColumnTransformer\")\\n\\n            logging.info(\\n                \"Exited get_data_transformer_object method of DataTransformation class\"\\n            )\\n            \\n            return preprocessor\\n        \\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n        \\n    \\n    def initiate_data_transformation(self, ) -> DataTransformationArtifact:\\n        \"\"\"\\n        Method Name :   initiate_data_transformation\\n        Description :   This method initiates the data transformation component for the pipeline \\n        \\n        Output      :   data transformer steps are performed and preprocessor object is created  \\n        On Failure  :   Write an exception log and then raise an exception\\n        \"\"\"\\n        try:\\n            if self.data_validation_artifact.validation_status:\\n                logging.info(\"Starting data transformation\")\\n                preprocessor = self.get_data_transformer_object()\\n                logging.info(\"Got the Preprocessor object\")\\n                \\n                train_df = DataTransformation.read_data(file_path=self.data_ingestion_artifact.trained_file_path)\\n                test_df = DataTransformation.read_data(file_path=self.data_ingestion_artifact.test_file_path)\\n                \\n                input_feature_train_df = train_df.drop(columns=[TARGET_COLUMN], axis=1)\\n                target_feature_train_df = train_df[TARGET_COLUMN]\\n                \\n                logging.info(\"Got train features and test features of Training dataset\")\\n                \\n                input_feature_train_df[\\'Gender\\'] = input_feature_train_df[\\'Gender\\'].replace(\"Fe Male\", \"Female\")\\n                \\n                logging.info(\"Spelling error correct the gender feature on the training dataset\")\\n                \\n                drop_cols = self._schema_config[\\'drop_columns\\']\\n                \\n                logging.info(\"drop the columns in drop_cols of Training dataset\")\\n                \\n                input_feature_train_df = drop_columns(df=input_feature_train_df, cols=drop_cols)\\n                \\n                input_feature_test_df = test_df.drop(columns=[TARGET_COLUMN], axis=1)\\n                target_feature_test_df = test_df[TARGET_COLUMN]\\n                \\n                input_feature_test_df[\\'Gender\\'] = input_feature_test_df[\\'Gender\\'].replace(\"Fe Male\", \"Female\")\\n                \\n                logging.info(\"Spelling error correct the gender feature on the testing dataset\")\\n                \\n                logging.info(\"drop the columns in drop_cols of Testing dataset\")\\n                \\n                input_feature_test_df = drop_columns(df=input_feature_test_df, cols=drop_cols)\\n                \\n                logging.info(\"Got train features and test features of Testing dataset\")\\n\\n                logging.info(\\n                    \"Applying preprocessing object on training dataframe and testing dataframe\"\\n                )\\n                \\n                input_feature_train_arr = preprocessor.fit_transform(input_feature_train_df)\\n                \\n                logging.info(\\n                    \"Used the preprocessor object to fit transform the train features\"\\n                )\\n                \\n                input_feature_test_arr = preprocessor.transform(input_feature_test_df)\\n                \\n                logging.info(\"Used the preprocessor object to transform the test features\")\\n\\n                logging.info(\"Applying SMOTEENN on Training dataset\")\\n                \\n                smt = SMOTEENN(sampling_strategy=\"minority\", random_state=RANDOM_STATE)\\n                \\n                input_feature_train_final, target_feature_train_final = smt.fit_resample(\\n                    input_feature_train_arr, target_feature_train_df\\n                )\\n\\n                logging.info(\"Applied SMOTEENN on training dataset\")\\n\\n                logging.info(\"Applying SMOTEENN on testing dataset\")\\n\\n                input_feature_test_final, target_feature_test_final = smt.fit_resample(\\n                    input_feature_test_arr, target_feature_test_df\\n                )\\n\\n                logging.info(\"Applied SMOTEENN on testing dataset\")\\n\\n                logging.info(\"Created train array and test array\")\\n\\n                train_arr = np.c_[\\n                    input_feature_train_final, np.array(target_feature_train_final)\\n                ]\\n\\n                test_arr = np.c_[\\n                    input_feature_test_final, np.array(target_feature_test_final)\\n                ]\\n\\n                save_object(self.data_transformation_config.transformed_object_file_path, preprocessor)\\n                save_numpy_array_data(self.data_transformation_config.transformed_train_file_path, array=train_arr)\\n                save_numpy_array_data(self.data_transformation_config.transformed_test_file_path, array=test_arr)\\n\\n                logging.info(\"Saved the preprocessor object\")\\n\\n                logging.info(\\n                    \"Exited initiate_data_transformation method of Data_Transformation class\"\\n                )\\n\\n                data_transformation_artifact = DataTransformationArtifact(\\n                    transformed_object_file_path=self.data_transformation_config.transformed_object_file_path,\\n                    transformed_train_file_path=self.data_transformation_config.transformed_train_file_path,\\n                    transformed_test_file_path=self.data_transformation_config.transformed_test_file_path\\n                )\\n                return data_transformation_artifact\\n            else:\\n                raise Exception(self.data_validation_artifact.message)\\n\\n                \\n        except Exception as e:\\n            raise TravelException(e, sys) from e', metadata={'source': 'test_repo\\\\travel_pack\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\nimport json\\n\\nimport pandas as pd\\nfrom evidently.model_profile import Profile\\nfrom evidently.model_profile.sections import DataDriftProfileSection\\n\\nfrom pandas import DataFrame\\n\\nfrom travel_pack.constants import SCHEMA_FILE_PATH\\nfrom travel_pack.constants import *\\nfrom travel_pack.exception import TravelException\\nfrom travel_pack.logger import logging\\n\\nfrom travel_pack.entity.config_entity import DataValidationConfig\\nfrom travel_pack.entity.artifact_entity import DataIngestionArtifact, DataValidationArtifact\\nfrom travel_pack.utils.main_utils import read_yaml_file, write_yaml_file\\n\\nclass DataValidation:\\n    def __init__(self, data_ingestion_artifact: DataIngestionArtifact, data_validation_config: DataValidationConfig):\\n        \"\"\"\\n        :param data_ingestion_artifact: Output reference of data ingestion artifact stage\\n        :param data_validation_config: configuration for data validation\\n        \"\"\"\\n        try:\\n            self.data_ingestion_artifact = data_ingestion_artifact\\n            self.data_validation_config = data_validation_config\\n            self._schema_config = read_yaml_file(file_path=SCHEMA_FILE_PATH)\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n        \\n    def validate_number_of_columns(self, dataframe: DataFrame) -> bool:\\n        \"\"\"\\n        Method Name :   validate_number_of_columns\\n        Description :   This method validates the number of columns\\n        \\n        Output      :   Returns bool value based on validation results\\n        On Failure  :   Write an exception log and then raise an exception\\n        \"\"\"\\n        try:\\n            status = len(dataframe.columns) == len(self._schema_config[\"columns\"])\\n            logging.info(f\"Is required column present: [{status}]\")\\n            return status\\n        except Exception as e:\\n            raise TravelException(e, sys)\\n\\n    def is_column_exist(self, df: DataFrame) -> bool:\\n        \"\"\"\\n        Method Name :   is_column_exist\\n        Description :   This method validates the existence of a numerical and categorical columns\\n        \\n        Output      :   Returns bool value based on validation results\\n        On Failure  :   Write an exception log and then raise an exception\\n        \"\"\"\\n        try:\\n            dataframe_columns = df.columns\\n            missing_numerical_columns = []\\n            missing_categorical_columns = []\\n            for column in self._schema_config[\"numerical_columns\"]:\\n                if column not in dataframe_columns:\\n                    missing_numerical_columns.append(column)\\n\\n            if len(missing_numerical_columns)>0:\\n                logging.info(f\"Missing numerical column: {missing_numerical_columns}\")\\n\\n\\n            for column in self._schema_config[\"categorical_columns\"]:\\n                if column not in dataframe_columns:\\n                    missing_categorical_columns.append(column)\\n\\n            if len(missing_categorical_columns)>0:\\n                logging.info(f\"Missing categorical column: {missing_categorical_columns}\")\\n\\n            return False if len(missing_categorical_columns)>0 or len(missing_numerical_columns)>0 else True\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n\\n    @staticmethod\\n    def read_data(file_path) -> DataFrame:\\n        try:\\n            return pd.read_csv(file_path)\\n        except Exception as e:\\n            raise TravelException(e, sys)\\n\\n    def detect_dataset_drift(self, reference_df: DataFrame, current_df: DataFrame, ) -> bool:\\n        \"\"\"\\n        Method Name :   detect_dataset_drift\\n        Description :   This method validates if drift is detected\\n        \\n        Output      :   Returns bool value based on validation results\\n        On Failure  :   Write an exception log and then raise an exception\\n        \"\"\"\\n        try:\\n            data_drift_profile = Profile(sections=[DataDriftProfileSection()])\\n\\n            data_drift_profile.calculate(reference_df, current_df)\\n\\n            report = data_drift_profile.json()\\n            json_report = json.loads(report)\\n\\n            write_yaml_file(file_path=self.data_validation_config.drift_report_file_path, content=json_report)\\n\\n            n_features = json_report[\"data_drift\"][\"data\"][\"metrics\"][\"n_features\"]\\n            n_drifted_features = json_report[\"data_drift\"][\"data\"][\"metrics\"][\"n_drifted_features\"]\\n\\n            logging.info(f\"{n_drifted_features}/{n_features} drift detected.\")\\n            drift_status = json_report[\"data_drift\"][\"data\"][\"metrics\"][\"dataset_drift\"]\\n            return drift_status\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n\\n    def initiate_data_validation(self) -> DataValidationArtifact:\\n        \"\"\"\\n        Method Name :   initiate_data_validation\\n        Description :   This method initiates the data validation component for the pipeline\\n        \\n        Output      :   Returns bool value based on validation results\\n        On Failure  :   Write an exception log and then raise an exception\\n        \"\"\"\\n\\n        try:\\n            validation_error_msg = \"\"\\n            logging.info(\"Starting data validation\")\\n            train_df, test_df = (DataValidation.read_data(file_path=self.data_ingestion_artifact.trained_file_path),\\n                                 DataValidation.read_data(file_path=self.data_ingestion_artifact.test_file_path))\\n\\n            status = self.validate_number_of_columns(dataframe=train_df)\\n            logging.info(f\"All required columns present in training dataframe: {status}\")\\n            if not status:\\n                validation_error_msg += f\"Columns are missing in training dataframe.\"\\n            status = self.validate_number_of_columns(dataframe=test_df)\\n\\n            logging.info(f\"All required columns present in testing dataframe: {status}\")\\n            if not status:\\n                validation_error_msg += f\"Columns are missing in test dataframe.\"\\n\\n            status = self.is_column_exist(df=train_df)\\n\\n            if not status:\\n                validation_error_msg += f\"Columns are missing in training dataframe.\"\\n            status = self.is_column_exist(df=test_df)\\n\\n            if not status:\\n                validation_error_msg += f\"columns are missing in test dataframe.\"\\n\\n            validation_status = len(validation_error_msg) == 0\\n\\n            if validation_status:\\n                drift_status = self.detect_dataset_drift(train_df, test_df)\\n                if drift_status:\\n                    logging.info(f\"Drift detected.\")\\n                    validation_error_msg = \"Drift detected\"\\n                else:\\n                    validation_error_msg = \"Drift not detected\"\\n            else:\\n                logging.info(f\"Validation_error: {validation_error_msg}\")\\n                \\n\\n            data_validation_artifact = DataValidationArtifact(\\n                validation_status=validation_status,\\n                message=validation_error_msg,\\n                drift_report_file_path=self.data_validation_config.drift_report_file_path\\n            )\\n\\n            logging.info(f\"Data validation artifact: {data_validation_artifact}\")\\n            return data_validation_artifact\\n        except Exception as e:\\n            raise TravelException(e, sys) from e', metadata={'source': 'test_repo\\\\travel_pack\\\\components\\\\data_validation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from travel_pack.entity.config_entity import ModelEvaluationConfig\\nfrom travel_pack.entity.artifact_entity import ModelTrainerArtifact, DataIngestionArtifact, ModelEvaluationArtifact\\nfrom sklearn.metrics import f1_score\\nfrom travel_pack.exception import TravelException\\nfrom travel_pack.logger import logging\\nfrom travel_pack.constants import TARGET_COLUMN\\nimport sys\\nimport pandas as pd\\nfrom typing import Optional\\nfrom travel_pack.entity.s3_estimator import TravelEstimator\\nfrom dataclasses import dataclass\\nfrom travel_pack.entity.estimator import TravelModel\\n\\n\\n@dataclass\\nclass EvaluateModelResponse:\\n    trained_model_f1_score: float\\n    best_model_f1_score: float\\n    is_model_accepted: bool\\n    difference: float\\n\\n\\nclass ModelEvaluation:\\n\\n    def __init__(self, model_eval_config: ModelEvaluationConfig, data_ingestion_artifact: DataIngestionArtifact,\\n                 model_trainer_artifact: ModelTrainerArtifact):\\n        try:\\n            self.model_eval_config = model_eval_config\\n            self.data_ingestion_artifact = data_ingestion_artifact\\n            self.model_trainer_artifact = model_trainer_artifact\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n\\n    def get_best_model(self) -> Optional[TravelEstimator]:\\n        \"\"\"\\n        Method Name :   get_best_model\\n        Description :   This function is used to get model in production\\n        \\n        Output      :   Returns model object if available in s3 storage\\n        On Failure  :   Write an exception log and then raise an exception\\n        \"\"\"\\n        try:\\n            bucket_name = self.model_eval_config.bucket_name\\n            model_path=self.model_eval_config.s3_model_key_path\\n            usvisa_estimator = TravelEstimator(bucket_name=bucket_name,\\n                                               model_path=model_path)\\n\\n            if usvisa_estimator.is_model_present(model_path=model_path):\\n                return usvisa_estimator\\n            return None\\n        except Exception as e:\\n            raise  TravelException(e,sys)\\n\\n    def evaluate_model(self) -> EvaluateModelResponse:\\n        \"\"\"\\n        Method Name :   evaluate_model\\n        Description :   This function is used to evaluate trained model \\n                        with production model and choose best model \\n        \\n        Output      :   Returns bool value based on validation results\\n        On Failure  :   Write an exception log and then raise an exception\\n        \"\"\"\\n        try:\\n            test_df = pd.read_csv(self.data_ingestion_artifact.test_file_path)\\n            test_df[\\'Gender\\'] = test_df[\\'Gender\\'].replace(\\'Fe Male\\', \\'Female\\') \\n\\n            x, y = test_df.drop(TARGET_COLUMN, axis=1), test_df[TARGET_COLUMN]\\n\\n            # trained_model = load_object(file_path=self.model_trainer_artifact.trained_model_file_path)\\n            trained_model_f1_score = self.model_trainer_artifact.metric_artifact.f1_score\\n\\n            best_model_f1_score=None\\n            best_model = self.get_best_model()\\n            if best_model is not None:\\n                y_hat_best_model = best_model.predict(x)\\n                best_model_f1_score = f1_score(y, y_hat_best_model)\\n            \\n            tmp_best_model_score = 0 if best_model_f1_score is None else best_model_f1_score\\n            result = EvaluateModelResponse(trained_model_f1_score=trained_model_f1_score,\\n                                           best_model_f1_score=best_model_f1_score,\\n                                           is_model_accepted=trained_model_f1_score > tmp_best_model_score,\\n                                           difference=trained_model_f1_score - tmp_best_model_score\\n                                           )\\n            logging.info(f\"Result: {result}\")\\n            return result\\n\\n        except Exception as e:\\n            raise TravelException(e, sys)\\n\\n    def initiate_model_evaluation(self) -> ModelEvaluationArtifact:\\n        \"\"\"\\n        Method Name :   initiate_model_evaluation\\n        Description :   This function is used to initiate all steps of the model evaluation\\n        \\n        Output      :   Returns model evaluation artifact\\n        On Failure  :   Write an exception log and then raise an exception\\n        \"\"\"  \\n        try:\\n            evaluate_model_response = self.evaluate_model()\\n            s3_model_path = self.model_eval_config.s3_model_key_path\\n\\n            model_evaluation_artifact = ModelEvaluationArtifact(\\n                is_model_accepted=evaluate_model_response.is_model_accepted,\\n                s3_model_path=s3_model_path,\\n                trained_model_path=self.model_trainer_artifact.trained_model_file_path,\\n                changed_accuracy=evaluate_model_response.difference)\\n\\n            logging.info(f\"Model evaluation artifact: {model_evaluation_artifact}\")\\n            return model_evaluation_artifact\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n', metadata={'source': 'test_repo\\\\travel_pack\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import sys\\n\\nfrom travel_pack.cloud_storage.aws_storage import SimpleStorageService\\nfrom travel_pack.exception import TravelException\\nfrom travel_pack.logger import logging\\nfrom travel_pack.entity.config_entity import ModelPusherConfig\\nfrom travel_pack.entity.artifact_entity import ModelPusherArtifact, ModelEvaluationArtifact\\nfrom travel_pack.entity.s3_estimator import TravelEstimator\\n\\n\\nclass ModelPusher:\\n    def __init__(self, model_evaluation_artifact: ModelEvaluationArtifact,\\n                 model_pusher_config: ModelPusherConfig):\\n        \"\"\"\\n        :param model_evaluation_artifact: Output reference of data evaluation artifact stage\\n        :param model_pusher_config: Configuration for model pusher\\n        \"\"\"\\n        self.s3 = SimpleStorageService()\\n        self.model_evaluation_artifact = model_evaluation_artifact\\n        self.model_pusher_config = model_pusher_config\\n        self.usvisa_estimator = TravelEstimator(bucket_name=model_pusher_config.bucket_name,\\n                                model_path=model_pusher_config.s3_model_key_path)\\n\\n    def initiate_model_pusher(self) -> ModelPusherArtifact:\\n        \"\"\"\\n        Method Name :   initiate_model_evaluation\\n        Description :   This function is used to initiate all steps of the model pusher\\n        \\n        Output      :   Returns model evaluation artifact\\n        On Failure  :   Write an exception log and then raise an exception\\n        \"\"\"\\n        logging.info(\"Entered initiate_model_pusher method of ModelTrainer class\")\\n\\n        try:\\n            logging.info(\"Uploading artifacts folder to s3 bucket\")\\n\\n            self.usvisa_estimator.save_model(from_file=self.model_evaluation_artifact.trained_model_path)\\n\\n\\n            model_pusher_artifact = ModelPusherArtifact(bucket_name=self.model_pusher_config.bucket_name,\\n                                                        s3_model_path=self.model_pusher_config.s3_model_key_path)\\n\\n            logging.info(\"Uploaded artifacts folder to s3 bucket\")\\n            logging.info(f\"Model pusher artifact: [{model_pusher_artifact}]\")\\n            logging.info(\"Exited initiate_model_pusher method of ModelTrainer class\")\\n            \\n            return model_pusher_artifact\\n        except Exception as e:\\n            raise TravelException(e, sys) from e', metadata={'source': 'test_repo\\\\travel_pack\\\\components\\\\model_pusher.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import sys\\nfrom typing import Tuple\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom pandas import DataFrame\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\\nfrom neuro_mf import ModelFactory\\n\\nfrom travel_pack.exception import TravelException\\nfrom travel_pack.logger import logging\\nfrom travel_pack.utils.main_utils import load_numpy_array_data, read_yaml_file, load_object, save_object\\nfrom travel_pack.entity.config_entity import ModelTrainerConfig\\nfrom travel_pack.entity.artifact_entity import DataTransformationArtifact, ModelTrainerArtifact, ClassificationMetricArtifact\\nfrom travel_pack.entity.estimator import TravelModel\\n\\nclass ModelTrainer:\\n    def __init__(self, data_transformation_artifact: DataTransformationArtifact,\\n                 model_trainer_config: ModelTrainerConfig):\\n        \"\"\"\\n        :param data_ingestion_artifact: Output reference of data ingestion artifact stage\\n        :param data_transformation_config: Configuration for data transformation\\n        \"\"\"\\n        self.data_transformation_artifact = data_transformation_artifact\\n        self.model_trainer_config = model_trainer_config\\n        \\n    def get_model_object_and_report(self, train: np.array, test: np.array) -> Tuple[object, object]:\\n        \"\"\"\\n        Method Name :   get_model_object_and_report\\n        Description :   This function uses neuro_mf to get the best model object and report of the best model\\n        \\n        Output      :   Returns metric artifact object and best model object\\n        On Failure  :   Write an exception log and then raise an exception\\n        \"\"\"\\n        try:\\n            logging.info(\"Using neuro_mf to get best model object and report\")\\n            model_factory = ModelFactory(model_config_path=self.model_trainer_config.model_config_file_path)\\n            \\n            x_train, y_train, x_test, y_test = train[:, :-1], train[:, -1], test[:, :-1], test[:, -1]\\n            \\n            best_model_detail = model_factory.get_best_model(\\n                X=x_train,y=y_train,base_accuracy=self.model_trainer_config.expected_accuracy\\n            )\\n            model_obj = best_model_detail.best_model\\n\\n            y_pred = model_obj.predict(x_test)\\n            \\n            accuracy = accuracy_score(y_test, y_pred) \\n            f1 = f1_score(y_test, y_pred)  \\n            precision = precision_score(y_test, y_pred)  \\n            recall = recall_score(y_test, y_pred)\\n            metric_artifact = ClassificationMetricArtifact(f1_score=f1, precision_score=precision, recall_score=recall)\\n            \\n            return best_model_detail, metric_artifact\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n        \\n    def initiate_model_trainer(self, ) -> ModelTrainerArtifact:\\n        logging.info(\"Entered initiate_model_trainer method of ModelTrainer class\")\\n        \"\"\"\\n        Method Name :   initiate_model_trainer\\n        Description :   This function initiates a model trainer steps\\n        \\n        Output      :   Returns model trainer artifact\\n        On Failure  :   Write an exception log and then raise an exception\\n        \"\"\"\\n        try:\\n            train_arr = load_numpy_array_data(file_path=self.data_transformation_artifact.transformed_train_file_path)\\n            test_arr = load_numpy_array_data(file_path=self.data_transformation_artifact.transformed_test_file_path)\\n            \\n            best_model_detail ,metric_artifact = self.get_model_object_and_report(train=train_arr, test=test_arr)\\n            \\n            preprocessing_obj = load_object(file_path=self.data_transformation_artifact.transformed_object_file_path)\\n\\n\\n            if best_model_detail.best_score < self.model_trainer_config.expected_accuracy:\\n                logging.info(\"No best model found with score more than base score\")\\n                raise Exception(\"No best model found with score more than base score\")\\n\\n            usvisa_model = TravelModel(preprocessing_object=preprocessing_obj,\\n                                       trained_model_object=best_model_detail.best_model)\\n            logging.info(\"Created usvisa model object with preprocessor and model\")\\n            logging.info(\"Created best model file path.\")\\n            save_object(self.model_trainer_config.trained_model_file_path, usvisa_model)\\n\\n            model_trainer_artifact = ModelTrainerArtifact(\\n                trained_model_file_path=self.model_trainer_config.trained_model_file_path,\\n                metric_artifact=metric_artifact,\\n            )\\n            logging.info(f\"Model trainer artifact: {model_trainer_artifact}\")\\n            return model_trainer_artifact\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n    ', metadata={'source': 'test_repo\\\\travel_pack\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\travel_pack\\\\components\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import boto3\\nimport os\\nfrom travel_pack.constants import AWS_SECRET_ACCESS_KEY_ENV_KEY, AWS_ACCESS_KEY_ID_ENV_KEY, REGION_NAME\\n\\nclass S3Client:\\n\\n    s3_client=None\\n    s3_resource = None\\n    def __init__(self, region_name=REGION_NAME):\\n        \"\"\" \\n        This Class gets aws credentials from env_variable and creates an connection with s3 bucket \\n        and raise exception when environment variable is not set\\n        \"\"\"\\n\\n        if S3Client.s3_resource==None or S3Client.s3_client==None:\\n            __access_key_id = os.getenv(AWS_ACCESS_KEY_ID_ENV_KEY, )\\n            __secret_access_key = os.getenv(AWS_SECRET_ACCESS_KEY_ENV_KEY, )\\n            if __access_key_id is None:\\n                raise Exception(f\"Environment variable: {AWS_ACCESS_KEY_ID_ENV_KEY} is not not set.\")\\n            if __secret_access_key is None:\\n                raise Exception(f\"Environment variable: {AWS_SECRET_ACCESS_KEY_ENV_KEY} is not set.\")\\n        \\n            S3Client.s3_resource = boto3.resource(\\'s3\\',\\n                                            aws_access_key_id=__access_key_id,\\n                                            aws_secret_access_key=__secret_access_key,\\n                                            region_name=region_name\\n                                            )\\n            S3Client.s3_client = boto3.client(\\'s3\\',\\n                                        aws_access_key_id=__access_key_id,\\n                                        aws_secret_access_key=__secret_access_key,\\n                                        region_name=region_name\\n                                        )\\n        self.s3_resource = S3Client.s3_resource\\n        self.s3_client = S3Client.s3_client\\n        ', metadata={'source': 'test_repo\\\\travel_pack\\\\configuration\\\\aws_connection.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\nfrom travel_pack.exception import TravelException\\nfrom travel_pack.logger import logging\\nfrom travel_pack.constants import DATABASE_NAME, MONGODB_URL_KEY\\nimport pymongo\\nimport certifi\\n\\nca = certifi.where()\\n\\nclass MongoDBClient:\\n    \"\"\"\\n    Class Name :   export_data_into_feature_store\\n    Description :   This method exports the dataframe from mongodb feature store as dataframe \\n    \\n    Output      :   connection to mongodb database\\n    On Failure  :   raises an exception\\n    \"\"\"\\n    client = None\\n    \\n    def __init__(self, database_name=DATABASE_NAME) -> None:\\n        try:\\n            if MongoDBClient.client is None:\\n                mongo_db_url = os.getenv(MONGODB_URL_KEY)\\n                if mongo_db_url is None:\\n                    raise Exception(f\"Environment key: {MONGODB_URL_KEY} is not set.\")\\n                MongoDBClient.client = pymongo.MongoClient(mongo_db_url, tlsCAFile=ca)\\n            self.client = MongoDBClient.client\\n            self.database = self.client[database_name]\\n            self.database_name = database_name\\n            logging.info(\"MongoDB connection successful\")\\n        except Exception as e:\\n            raise TravelException(e, sys) from e', metadata={'source': 'test_repo\\\\travel_pack\\\\configuration\\\\mongo_db_connection.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\travel_pack\\\\configuration\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\n\\nPIPELINE_NAME: str = \"travel_pack\"\\nARTIFACT_DIR: str = \"artifacts\"\\n\\nMONGODB_URL_KEY = \"MONGODB_URL\"\\n\\nDATABASE_NAME = \"iNeuron\"\\nCOLLECTION_NAME = \"travel\"\\n\\nMODEL_FILE_NAME = \"model.pkl\"\\nPREROCESSING_OBEJCT_FILE_NAME = \"preprocessing.pkl\"\\nTARGET_COLUMN = \"ProdTaken\"\\nRANDOM_STATE = 42\\n\\n# common file name\\nFILE_NAME: str = \"travel.csv\"\\nTRAIN_FILE_NAME: str = \"train.csv\"\\nTEST_FILE_NAME: str = \"test.csv\"\\nSCHEMA_FILE_PATH: str = os.path.join(\"config\", \"schema.yaml\")\\n\\nAWS_ACCESS_KEY_ID_ENV_KEY = \"AWS_ACCESS_KEY_ID\"\\nAWS_SECRET_ACCESS_KEY_ENV_KEY = \"AWS_SECRET_ACCESS_KEY\"\\nREGION_NAME = \"us-east-1\"\\n\\n\\n\"\"\"\\nData Ingestion related constant start with DATA_INGESTION VAR NAME\\n\"\"\"\\nDATA_INGESTION_COLLECTION_NAME: str = \"travel\"\\nDATA_INGESTION_DIR_NAME: str = \"data_ingestion\"\\nDATA_INGESTION_FEATURE_STORE_DIR: str = \"feature_store\"\\nDATA_INGESTION_INGESTED_DIR: str = \"ingested\"\\nDATA_INGESTION_TRAIN_TEST_SPLIT_RATIO: float = 0.2\\n\\n\\n\"\"\"\\nData Validation related constant start with DATA_VALIDATION VAR NAME\\n\"\"\"\\nDATA_VALIDATION_DIR_NAME: str = \"data_validation\"\\nDATA_VALIDATION_DRIFT_REPORT_DIR: str = \"drift_report\"\\nDATA_VALIDATION_DRIFT_REPORT_FILE_NAME: str = \"report.yaml\"\\n\\n\\n\"\"\"\\nData Transformation related constant start with DATA_TRANSFORMATION VAR NAME\\n\"\"\"\\nDATA_TRANSFORMATION_DIR_NAME: str = \"data_transformation\"\\nDATA_TRANSFORMATION_TRANSFORMED_DATA_DIR: str = \"transformed\"\\nDATA_TRANSFORMATION_TRANSFORMED_OBJECT_DIR: str = \"transformed_object\"\\n\\n\\n\"\"\"\\nModel Trainer related constant start with MODEL TRAINER VAR NAME\\n\"\"\"\\nMODEL_TRAINER_DIR_NAME: str = \"model_trainer\"\\nMODEL_TRAINER_TRAINED_MODEL_DIR: str = \"trained_model\"\\nMODEL_TRAINER_TRAINED_MODEL_NAME: str = \"model.pkl\"\\nMODEL_TRAINER_EXPECTED_SCORE: float = 0.6\\nMODEL_TRAINER_MODEL_CONFIG_FILE_PATH: str = os.path.join(\"config\", \"model.yaml\")\\n\\n\\nMODEL_EVALUATION_CHANGED_THRESHOLD_SCORE: float = 0.02\\nMODEL_BUCKET_NAME = \"travel-model2024\"\\nMODEL_PUSHER_S3_KEY = \"model-registry\"\\n\\nAPP_HOST = \"0.0.0.0\"\\nAPP_PORT = 8080', metadata={'source': 'test_repo\\\\travel_pack\\\\constants\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\nfrom travel_pack.exception import TravelException\\nfrom travel_pack.logger import logging\\nfrom travel_pack.configuration.mongo_db_connection import MongoDBClient\\nfrom travel_pack.constants import DATABASE_NAME\\nimport pandas as pd\\nimport numpy as np\\nfrom typing import Optional\\n\\nclass TravelData:\\n    \"\"\"\\n    This class help to expert entire mongo db record as pandas dataframe\\n    \"\"\"\\n    def __init__(self):\\n        \"\"\"\\n        \"\"\"\\n        try:\\n            self.mongo_client = MongoDBClient(database_name=DATABASE_NAME)\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n        \\n        \\n    def export_collection_as_dataframe(self, collection_name:str, database_name:Optional[str]=None) -> pd.DataFrame:\\n        try:\\n            \"\"\"\\n            export entire collectin as dataframe:\\n            return pd.DataFrame of collection\\n            \"\"\"\\n            if database_name is None:\\n                collection = self.mongo_client.database[collection_name]\\n            else:\\n                collection = self.mongo_client[database_name][collection_name]\\n                \\n            df = pd.DataFrame(list(collection.find()))\\n            if \"_id\" in df.columns.to_list():\\n                df = df.drop(columns=[\"_id\"], axis=1)\\n            df.replace({\"na\":np.nan}, inplace=True)\\n            return df\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n', metadata={'source': 'test_repo\\\\travel_pack\\\\data_access\\\\travel_data.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\travel_pack\\\\data_access\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from dataclasses import dataclass\\n\\n@dataclass\\nclass DataIngestionArtifact:\\n    trained_file_path: str\\n    test_file_path: str\\n    \\n@dataclass\\nclass DataValidationArtifact:\\n    validation_status: bool\\n    message: str\\n    drift_report_file_path: str\\n\\n@dataclass\\nclass DataTransformationArtifact:\\n    transformed_object_file_path: str\\n    transformed_train_file_path: str\\n    transformed_test_file_path: str\\n    \\n\\n@dataclass\\nclass ClassificationMetricArtifact:\\n    f1_score: float\\n    precision_score: float\\n    recall_score: float\\n    \\n@dataclass\\nclass ModelTrainerArtifact:\\n    trained_model_file_path: str\\n    metric_artifact: ClassificationMetricArtifact\\n    \\n\\n@dataclass\\nclass ModelEvaluationArtifact:\\n    is_model_accepted: bool\\n    changed_accuracy: float\\n    s3_model_path: str\\n    trained_model_path: str\\n    \\n    \\n@dataclass\\nclass ModelPusherArtifact:\\n    bucket_name: str\\n    s3_model_path: str', metadata={'source': 'test_repo\\\\travel_pack\\\\entity\\\\artifact_entity.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\nfrom travel_pack.constants import *\\nfrom datetime import datetime\\nfrom dataclasses import dataclass\\n\\nTIMESTAMP: str = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\\n\\n@dataclass\\nclass TrainingPipelineConfig:\\n    pipeline_name: str = PIPELINE_NAME\\n    artifact_dir = os.path.join(ARTIFACT_DIR, TIMESTAMP)\\n    timestamp: str = TIMESTAMP\\n    \\ntraining_pipeline_config: TrainingPipelineConfig = TrainingPipelineConfig()\\n\\n@dataclass\\nclass DataIngestionConfig:\\n    data_ingestion_dir: str = os.path.join(training_pipeline_config.artifact_dir, DATA_INGESTION_DIR_NAME)\\n    feature_store_file_path: str = os.path.join(data_ingestion_dir, DATA_INGESTION_FEATURE_STORE_DIR, FILE_NAME)\\n    training_file_path: str = os.path.join(data_ingestion_dir, DATA_INGESTION_INGESTED_DIR, TRAIN_FILE_NAME)\\n    testing_file_path: str = os.path.join(data_ingestion_dir, DATA_INGESTION_INGESTED_DIR, TEST_FILE_NAME)\\n    train_test_split_ratio: float = DATA_INGESTION_TRAIN_TEST_SPLIT_RATIO\\n    collection_name: str = COLLECTION_NAME\\n    \\n@dataclass\\nclass DataValidationConfig:\\n    data_validation_dir: str = os.path.join(training_pipeline_config.artifact_dir, DATA_VALIDATION_DIR_NAME)\\n    drift_report_file_path: str = os.path.join(data_validation_dir, DATA_VALIDATION_DRIFT_REPORT_DIR,\\n                                               DATA_VALIDATION_DRIFT_REPORT_FILE_NAME)\\n    \\n    \\n@dataclass\\nclass DataTransformationConfig:\\n    data_transformation_dir: str = os.path.join(training_pipeline_config.artifact_dir, DATA_TRANSFORMATION_DIR_NAME)\\n    transformed_train_file_path: str = os.path.join(data_transformation_dir, DATA_TRANSFORMATION_TRANSFORMED_DATA_DIR,\\n                                                    TRAIN_FILE_NAME.replace(\"csv\", \"npy\"))\\n    transformed_test_file_path: str = os.path.join(data_transformation_dir, DATA_TRANSFORMATION_TRANSFORMED_DATA_DIR,\\n                                                   TEST_FILE_NAME.replace(\"csv\", \"npy\"))\\n    transformed_object_file_path: str = os.path.join(data_transformation_dir,\\n                                                     DATA_TRANSFORMATION_TRANSFORMED_OBJECT_DIR,\\n                                                     PREROCESSING_OBEJCT_FILE_NAME)\\n    \\n@dataclass\\nclass ModelTrainerConfig:\\n    model_trainer_dir: str = os.path.join(training_pipeline_config.artifact_dir, MODEL_TRAINER_DIR_NAME)\\n    trained_model_file_path: str = os.path.join(model_trainer_dir, MODEL_TRAINER_TRAINED_MODEL_DIR, MODEL_FILE_NAME)\\n    expected_accuracy: float = MODEL_TRAINER_EXPECTED_SCORE\\n    model_config_file_path: str = MODEL_TRAINER_MODEL_CONFIG_FILE_PATH\\n    \\n    \\n@dataclass\\nclass ModelEvaluationConfig:\\n    changed_threshold_score: float = MODEL_EVALUATION_CHANGED_THRESHOLD_SCORE\\n    bucket_name: str = MODEL_BUCKET_NAME\\n    s3_model_key_path: str = MODEL_FILE_NAME\\n    \\n\\n@dataclass\\nclass ModelPusherConfig:\\n    bucket_name: str = MODEL_BUCKET_NAME\\n    s3_model_key_path: str = MODEL_FILE_NAME\\n    \\n    \\n@dataclass\\nclass TravelPredictorConfig:\\n    model_file_path: str = MODEL_FILE_NAME\\n    model_bucket_name: str = MODEL_BUCKET_NAME', metadata={'source': 'test_repo\\\\travel_pack\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from travel_pack.exception import TravelException\\nfrom travel_pack.logger import logging\\nfrom sklearn.pipeline import Pipeline\\nimport sys\\nfrom pandas import DataFrame\\n\\n\\nclass TravelModel:\\n    def __init__(self, preprocessing_object: Pipeline, trained_model_object: object):\\n        \"\"\"\\n        :param preprocessing_object: Input Object of preprocesser\\n        :param trained_model_object: Input Object of trained model \\n        \"\"\"\\n        self.preprocessing_object = preprocessing_object\\n        self.trained_model_object = trained_model_object\\n\\n    def predict(self, dataframe: DataFrame) -> DataFrame:\\n        \"\"\"\\n        Function accepts raw inputs and then transformed raw input using preprocessing_object\\n        which guarantees that the inputs are in the same format as the training data\\n        At last it performs prediction on transformed features\\n        \"\"\"\\n        logging.info(\"Entered predict method of UTruckModel class\")\\n\\n        try:\\n            logging.info(\"Using the trained model to get predictions\")\\n\\n            transformed_feature = self.preprocessing_object.transform(dataframe)\\n\\n            logging.info(\"Used the trained model to get predictions\")\\n            return self.trained_model_object.predict(transformed_feature)\\n\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n\\n    def __repr__(self):\\n        return f\"{type(self.trained_model_object).__name__}()\"\\n\\n    def __str__(self):\\n        return f\"{type(self.trained_model_object).__name__}()\"', metadata={'source': 'test_repo\\\\travel_pack\\\\entity\\\\estimator.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from travel_pack.cloud_storage.aws_storage import SimpleStorageService\\nfrom travel_pack.exception import TravelException\\nfrom travel_pack.entity.estimator import TravelModel\\nimport sys\\nfrom pandas import DataFrame\\n\\n\\nclass TravelEstimator:\\n    \"\"\"\\n    This class is used to save and retrieve us_visas model in s3 bucket and to do prediction\\n    \"\"\"\\n\\n    def __init__(self,bucket_name,model_path,):\\n        \"\"\"\\n        :param bucket_name: Name of your model bucket\\n        :param model_path: Location of your model in bucket\\n        \"\"\"\\n        self.bucket_name = bucket_name\\n        self.s3 = SimpleStorageService()\\n        self.model_path = model_path\\n        self.loaded_model:TravelModel=None\\n\\n\\n    def is_model_present(self,model_path):\\n        try:\\n            return self.s3.s3_key_path_available(bucket_name=self.bucket_name, s3_key=model_path)\\n        except TravelException as e:\\n            print(e)\\n            return False\\n\\n    def load_model(self,)->TravelModel:\\n        \"\"\"\\n        Load the model from the model_path\\n        :return:\\n        \"\"\"\\n\\n        return self.s3.load_model(self.model_path,bucket_name=self.bucket_name)\\n\\n    def save_model(self,from_file,remove:bool=False)->None:\\n        \"\"\"\\n        Save the model to the model_path\\n        :param from_file: Your local system model path\\n        :param remove: By default it is false that mean you will have your model locally available in your system folder\\n        :return:\\n        \"\"\"\\n        try:\\n            self.s3.upload_file(from_file,\\n                                to_filename=self.model_path,\\n                                bucket_name=self.bucket_name,\\n                                remove=remove\\n                                )\\n        except Exception as e:\\n            raise TravelException(e, sys)\\n\\n\\n    def predict(self,dataframe:DataFrame):\\n        \"\"\"\\n        :param dataframe:\\n        :return:\\n        \"\"\"\\n        try:\\n            if self.loaded_model is None:\\n                self.loaded_model = self.load_model()\\n            return self.loaded_model.predict(dataframe=dataframe)\\n        except Exception as e:\\n            raise TravelException(e, sys)', metadata={'source': 'test_repo\\\\travel_pack\\\\entity\\\\s3_estimator.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\travel_pack\\\\entity\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\n\\ndef error_message_detail(error, error_detail:sys):\\n    _, _, exc_tb = error_detail.exc_info()\\n    file_name = exc_tb.tb_frame.f_code.co_filename\\n    error_message = \"Error occurred python script name [{0} line number [{1}] error message [{2}]\".format(\\n        file_name, exc_tb.tb_lineno, str(error)\\n    )\\n    \\n    return error_message\\n\\n\\nclass TravelException(Exception):\\n    def __init__(self, error_message, error_detail):\\n        \"\"\"\\n        :param error_message: error message in string format\\n        \"\"\"\\n        super().__init__(error_message)\\n        self.error_message = error_message_detail(\\n            error_message, error_detail=error_detail\\n        )\\n        \\n    def __str__(self):\\n        return self.error_message', metadata={'source': 'test_repo\\\\travel_pack\\\\exception\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import logging\\nimport os\\n\\nfrom from_root import from_root\\nfrom datetime import datetime\\n\\n\\nLOG_FILE = f\"{datetime.now().strftime(\\'%m_%d_%Y_%H_%M_%S\\')}.log\"\\n\\nlog_dir = \\'logs\\'\\n\\nlogs_path = os.path.join(from_root(), log_dir, LOG_FILE)\\n\\nos.makedirs(log_dir, exist_ok=True)\\n\\nlogging.basicConfig(\\n    filename=logs_path,\\n    format=\"[ %(asctime)s ] %(name)s - %(levelname)s - %(message)s\",\\n    level=logging.DEBUG\\n)', metadata={'source': 'test_repo\\\\travel_pack\\\\logger\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom travel_pack.entity.config_entity import TravelPredictorConfig\\nfrom travel_pack.entity.s3_estimator import TravelEstimator\\nfrom travel_pack.exception import TravelException\\nfrom travel_pack.logger import logging\\nfrom travel_pack.utils.main_utils import read_yaml_file\\nfrom pandas import DataFrame\\n\\n\\nclass TravelData:\\n    def __init__(self,\\n                 Age,\\n                 CityTier,\\n                 DurationOfPitch,\\n                 NumberOfPersonVisiting,\\n                 NumberOfFollowups,\\n                 PreferredPropertyStar,\\n                 NumberOfTrips,\\n                 Passport,\\n                 PitchSatisfactionScore,\\n                 OwnCar,\\n                 NumberOfChildrenVisiting,\\n                 MonthlyIncome,\\n                 TypeofContact,\\n                 Occupation,\\n                 Gender,\\n                 ProductPitched,\\n                 MaritalStatus,\\n                 Designation,\\n                 ):\\n        \"\"\"\\n        Travel Data constructor\\n        Input: all features of the trained model for prediction\\n        \"\"\"\\n        try:\\n            self.Age = Age\\n            self.CityTier = CityTier\\n            self.DurationOfPitch = DurationOfPitch\\n            self.NumberOfPersonVisiting = NumberOfPersonVisiting\\n            self.NumberOfFollowups = NumberOfFollowups\\n            self.PreferredPropertyStar = PreferredPropertyStar\\n            self.NumberOfTrips = NumberOfTrips\\n            self.Passport = Passport\\n            self.PitchSatisfactionScore = PitchSatisfactionScore\\n            self.OwnCar = OwnCar\\n            self.NumberOfChildrenVisiting = NumberOfChildrenVisiting\\n            self.MonthlyIncome = MonthlyIncome\\n            self.TypeofContact = TypeofContact\\n            self.Occupation = Occupation\\n            self.Gender = Gender\\n            self.ProductPitched = ProductPitched\\n            self.MaritalStatus = MaritalStatus\\n            self.Designation = Designation\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n        \\n    def get_travel_input_data_frame(self) -> DataFrame:\\n        \"\"\"\\n        This function returns a DataFrame from TravelData class input\\n        \"\"\"\\n        try:\\n            \\n            travel_input_dict = self.get_travel_data_as_dict()\\n            return DataFrame(travel_input_dict)\\n        \\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n        \\n        \\n    def get_travel_data_as_dict(self):\\n        \"\"\"\\n        This function returns a dictionary from TravelData class input \\n        \"\"\"\\n        logging.info(\"Entered get_travel_data_as_dict method as TravelData class\")\\n        \\n        try:\\n            input_data = {\\n                \"Age\": [self.Age],\\n                \"CityTier\": [self.CityTier],\\n                \"DurationOfPitch\": [self.DurationOfPitch],\\n                \"NumberOfPersonVisiting\": [self.NumberOfPersonVisiting],\\n                \"NumberOfFollowups\": [self.NumberOfFollowups],\\n                \"PreferredPropertyStar\": [self.PreferredPropertyStar],\\n                \"NumberOfTrips\": [self.NumberOfTrips],\\n                \"Passport\": [self.Passport],\\n                \"PitchSatisfactionScore\": [self.PitchSatisfactionScore],\\n                \"OwnCar\": [self.OwnCar],\\n                \"NumberOfChildrenVisiting\": [self.NumberOfChildrenVisiting],\\n                \"MonthlyIncome\": [self.MonthlyIncome],\\n                \"TypeofContact\": [self.TypeofContact],\\n                \"Occupation\": [self.Occupation],\\n                \"Gender\": [self.Gender],\\n                \"ProductPitched\": [self.ProductPitched],\\n                \"MaritalStatus\": [self.MaritalStatus],\\n                \"Designation\": [self.Designation],\\n            }\\n            \\n            logging.info(\"Created travel data dict\")\\n            \\n            logging.info(\"Exited get_travel_data_as_dict method as TravelData class\")\\n            \\n            return input_data\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n        \\nclass TravelClassifier:\\n    def __init__(self, prediction_pipeline_config: TravelPredictorConfig = TravelPredictorConfig(),) -> None:\\n        \"\"\"\\n        :param prediction_pipeline_config: Configuration for prediction the value\\n        \"\"\"\\n        try:\\n            # self.schema_config = read_yaml_file(SCHEMA_FILE_PATH)\\n            self.prediction_pipeline_config = prediction_pipeline_config\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n        \\n    def predict(self, dataframe) -> str:\\n        \"\"\"\\n        This is the method of TravelClassifier\\n        Returns: Prediction in string format\\n        \"\"\"\\n        try:\\n            logging.info(\"Entered predict method of TravelClassifier class\")\\n            model = TravelEstimator(\\n                bucket_name=self.prediction_pipeline_config.model_bucket_name,\\n                model_path=self.prediction_pipeline_config.model_file_path,\\n            )\\n            result = model.predict(dataframe)\\n            \\n            return result\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n        ', metadata={'source': 'test_repo\\\\travel_pack\\\\pipeline\\\\prediction_pipeline.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\n\\nfrom travel_pack.exception import TravelException\\nfrom travel_pack.logger import logging\\n\\nfrom travel_pack.components.data_ingestion import DataIngestion\\nfrom travel_pack.components.data_validation import DataValidation\\nfrom travel_pack.components.data_transformation import DataTransformation\\nfrom travel_pack.components.model_trainer import ModelTrainer\\nfrom travel_pack.components.model_evaluation import ModelEvaluation\\nfrom travel_pack.components.model_pusher import ModelPusher\\n\\nfrom travel_pack.entity.config_entity import (DataIngestionConfig,\\n                                              DataValidationConfig,\\n                                              DataTransformationConfig,\\n                                              ModelTrainerConfig,\\n                                              ModelEvaluationConfig,\\n                                              ModelPusherConfig)\\n\\nfrom travel_pack.entity.artifact_entity import (DataIngestionArtifact,\\n                                                DataValidationArtifact,\\n                                                DataTransformationArtifact,\\n                                                ModelTrainerArtifact,\\n                                                ModelEvaluationArtifact,\\n                                                ModelPusherArtifact)\\n\\n\\nclass TrainPipeline:\\n    def __init__(self):\\n        self.data_ingestion_config = DataIngestionConfig()\\n        self.data_validation_config = DataValidationConfig()\\n        self.data_transformation_config = DataTransformationConfig()\\n        self.model_trainer_config = ModelTrainerConfig()\\n        self.model_evaluation_config = ModelEvaluationConfig()\\n        self.model_pusher_config = ModelPusherConfig()\\n        \\n    def start_data_ingestion(self) -> DataIngestionArtifact:\\n        \"\"\"\\n        This method of TrainPipeline class is responsible for starting data ingestion component\\n        \"\"\"\\n        try:\\n            logging.info(\"Entered the start_data_ingestion method of TrainPipeline class\")\\n            logging.info(\"Getting the data from mongodb\")\\n            data_ingestion = DataIngestion(data_ingestion_config=self.data_ingestion_config)\\n            data_ingestion_artifact = data_ingestion.initiate_data_ingestion()\\n            logging.info(\"Got the train_set and test_set from mongodb\")\\n            logging.info(\\n                \"Exited the start_data_ingestion method of TrainPipeline class\"\\n            )\\n            return data_ingestion_artifact\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n        \\n    def start_data_validation(self, data_ingestion_artifact: DataIngestionArtifact) -> DataValidationArtifact:\\n        \"\"\"\\n        This method of TrainPipeline class is responsible for starting data validation component\\n        \"\"\"\\n        logging.info(\"Entered the start_data_validation method of TrainPipeline class\")\\n        try:\\n            data_validation = DataValidation(data_ingestion_artifact=data_ingestion_artifact,\\n                                             data_validation_config=self.data_validation_config)\\n            \\n            data_validation_artifact = data_validation.initiate_data_validation()\\n            \\n            logging.info(\"Performed the data validation operation\")\\n            \\n            logging.info(\\n                \"Exited the start_data_validation method of TrainPipeline class\"\\n            )\\n            \\n            return data_validation_artifact\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n        \\n    def start_data_transformation(self,\\n                                  data_ingestion_artifact: DataIngestionArtifact, \\n                                  data_validation_artifact: DataValidationArtifact) -> DataTransformationArtifact:\\n        \"\"\"\\n        This method of TrainPipeline class is responsible for starting data transformation component\\n        \"\"\"\\n        try:\\n            data_transformation = DataTransformation(data_ingestion_artifact=data_ingestion_artifact,\\n                                                     data_transformation_config=self.data_transformation_config,\\n                                                     data_validation_artifact=data_validation_artifact)\\n            data_transformation_artifact = data_transformation.initiate_data_transformation()\\n            return data_transformation_artifact\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n        \\n        \\n    def start_model_trainer(self, data_transformation_artifact: DataTransformationArtifact) -> ModelTrainerArtifact:\\n        \"\"\"\\n        This method of TrainPipeline class is responsible for starting model training\\n        \"\"\"\\n        try:\\n            model_trainer = ModelTrainer(data_transformation_artifact=data_transformation_artifact,\\n                                         model_trainer_config=self.model_trainer_config\\n                                         )\\n            model_trainer_artifact = model_trainer.initiate_model_trainer()\\n            return model_trainer_artifact\\n\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n        \\n        \\n    def start_model_evaluation(self, data_ingestion_artifact: DataIngestionArtifact,\\n                               model_trainer_artifact: ModelTrainerArtifact) -> ModelEvaluationArtifact:\\n        \"\"\"\\n        This method of TrainPipeline class is responsible for starting modle evaluation\\n        \"\"\"\\n        try:\\n            model_evaluation = ModelEvaluation(model_eval_config=self.model_evaluation_config,\\n                                               data_ingestion_artifact=data_ingestion_artifact,\\n                                               model_trainer_artifact=model_trainer_artifact)\\n            model_evaluation_artifact = model_evaluation.initiate_model_evaluation()\\n            return model_evaluation_artifact\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n        \\n        \\n    def start_model_pusher(self, model_evaluation_artifact: ModelEvaluationArtifact) -> ModelPusherArtifact:\\n        \"\"\"\\n        This method of TrainPipeline class is responsible for starting model pushing\\n        \"\"\"\\n        try:\\n            model_pusher = ModelPusher(model_evaluation_artifact=model_evaluation_artifact,\\n                                       model_pusher_config=self.model_pusher_config\\n                                       )\\n            model_pusher_artifact = model_pusher.initiate_model_pusher()\\n            return model_pusher_artifact\\n        except Exception as e:\\n            raise TravelException(e, sys) from e\\n\\n        \\n        \\n    def run_pipeline(self, ) -> None:\\n        \"\"\"\\n        This method of TrainPipeline class is responsible for running complete pipeline\\n        \"\"\"\\n        try:\\n            data_ingestion_artifact = self.start_data_ingestion()\\n            \\n            data_validation_artifact = self.start_data_validation(data_ingestion_artifact=data_ingestion_artifact)\\n            \\n            data_transformation_artifact = self.start_data_transformation(data_ingestion_artifact=data_ingestion_artifact,\\n                                                                          data_validation_artifact=data_validation_artifact)\\n\\n            model_trainer_artifact = self.start_model_trainer(data_transformation_artifact=data_transformation_artifact)\\n            \\n            model_evaluation_artifact = self.start_model_evaluation(data_ingestion_artifact=data_ingestion_artifact,\\n                                                                    model_trainer_artifact=model_trainer_artifact)\\n            \\n            if not model_evaluation_artifact.is_model_accepted:\\n                logging.info(f\"Model no accepted.\")\\n                return None\\n            model_pusher_artifact = self.start_model_pusher(model_evaluation_artifact=model_evaluation_artifact)\\n        \\n        except Exception as e:\\n            raise TravelException(e, sys) from e', metadata={'source': 'test_repo\\\\travel_pack\\\\pipeline\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\travel_pack\\\\pipeline\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\n\\nimport yaml\\nimport dill\\nimport numpy as np\\nfrom pandas import DataFrame\\nfrom travel_pack.exception import TravelException\\nfrom travel_pack.logger import logging\\n\\n\\ndef read_yaml_file(file_path: str) -> dict:\\n    try:\\n        with open(file_path, \"rb\") as yaml_file:\\n            return yaml.safe_load(yaml_file)\\n        \\n    except Exception as e:\\n        raise TravelException(e, sys) from e\\n\\ndef write_yaml_file(file_path: str, content: object, replace: bool = False) -> None:\\n    try:\\n        if replace:\\n            if os.path.exists(file_path):\\n                os.remove(file_path)\\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n        with open(file_path, \"w\") as file:\\n            yaml.dump(content, file)\\n    except Exception as e:\\n        raise TravelException(e, sys) from e\\n    \\ndef load_object(file_path: str) -> object:\\n    logging.info(\"Entered the load_object method of utils\")\\n\\n    try:\\n\\n        with open(file_path, \"rb\") as file_obj:\\n            obj = dill.load(file_obj)\\n\\n        logging.info(\"Exited the load_object method of utils\")\\n\\n        return obj\\n\\n    except Exception as e:\\n        raise TravelException(e, sys) from e\\n\\n      \\ndef save_numpy_array_data(file_path: str, array: np.array):\\n    \"\"\"\\n    Save numpy array data to file\\n    file_path: str location of file to save\\n    array: np.array data to save\\n    \"\"\"\\n    try:\\n        dir_path = os.path.dirname(file_path)\\n        os.makedirs(dir_path, exist_ok=True)\\n        with open(file_path, \\'wb\\') as file_obj:\\n            np.save(file_obj, array)\\n    except Exception as e:\\n        raise TravelException(e, sys) from e\\n    \\ndef load_numpy_array_data(file_path: str) -> np.array:\\n    \"\"\"\\n    load numpy array data from file\\n    file_path: str location of file to load\\n    return: np.array data loaded\\n    \"\"\"\\n    try:\\n        with open(file_path, \\'rb\\') as file_obj:\\n            return np.load(file_obj)\\n    except Exception as e:\\n        raise TravelException(e, sys) from e\\n\\n    \\ndef save_object(file_path: str, obj: object) -> None:\\n    logging.info(\"Entered the save_object method of utils\")\\n\\n    try:\\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n        with open(file_path, \"wb\") as file_obj:\\n            dill.dump(obj, file_obj)\\n\\n        logging.info(\"Exited the save_object method of utils\")\\n\\n    except Exception as e:\\n        raise TravelException(e, sys) from e\\n    \\n    \\ndef drop_columns(df: DataFrame, cols: list) -> DataFrame:\\n    \"\"\"\\n    drop the columns form a pandas DataFrame\\n    df: pandas DataFrame\\n    cols: list of columns to be dropped\\n    \"\"\"\\n    logging.info(\"Entered drop_columns methon of utils\")\\n    try:\\n        df = df.drop(columns=cols, axis=1)\\n        \\n        logging.info(\"Exited the drop_columns method of utils\")\\n        \\n        return df\\n    except Exception as e:\\n        raise TravelException(e, sys) from e', metadata={'source': 'test_repo\\\\travel_pack\\\\utils\\\\main_utils.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\travel_pack\\\\utils\\\\__init__.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunkings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON,\n",
    "                                                                  chunk_size=2000,\n",
    "                                                                  chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knowledge base (vector DB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 8.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 10.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m vectordb\u001b[38;5;241m.\u001b[39mpersist()\n",
      "File \u001b[1;32mc:\\Users\\durge\\anaconda3\\envs\\llmapp\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:592\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    591\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 592\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\durge\\anaconda3\\envs\\llmapp\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:556\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \n\u001b[0;32m    530\u001b[0m \u001b[38;5;124;03mIf a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    547\u001b[0m chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    548\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[0;32m    549\u001b[0m     embedding_function\u001b[38;5;241m=\u001b[39membedding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    555\u001b[0m )\n\u001b[1;32m--> 556\u001b[0m \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[1;32mc:\\Users\\durge\\anaconda3\\envs\\llmapp\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:176\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[1;32mc:\\Users\\durge\\anaconda3\\envs\\llmapp\\lib\\site-packages\\langchain\\embeddings\\openai.py:508\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \n\u001b[0;32m    498\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;124;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m--> 508\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeployment\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\durge\\anaconda3\\envs\\llmapp\\lib\\site-packages\\langchain\\embeddings\\openai.py:358\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    355\u001b[0m     _iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tokens), _chunk_size)\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[1;32m--> 358\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43membed_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invocation_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m     batched_embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    365\u001b[0m results: List[List[List[\u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(texts))]\n",
      "File \u001b[1;32mc:\\Users\\durge\\anaconda3\\envs\\llmapp\\lib\\site-packages\\langchain\\embeddings\\openai.py:107\u001b[0m, in \u001b[0;36membed_with_retry\u001b[1;34m(embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m     response \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _check_response(response)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_embed_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\durge\\anaconda3\\envs\\llmapp\\lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\durge\\anaconda3\\envs\\llmapp\\lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\durge\\anaconda3\\envs\\llmapp\\lib\\site-packages\\tenacity\\__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    323\u001b[0m     retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[1;32m--> 325\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait:\n",
      "File \u001b[1;32mc:\\Users\\durge\\anaconda3\\envs\\llmapp\\lib\\site-packages\\tenacity\\__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[1;32m--> 158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\durge\\anaconda3\\envs\\llmapp\\lib\\concurrent\\futures\\_base.py:437\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\durge\\anaconda3\\envs\\llmapp\\lib\\concurrent\\futures\\_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\durge\\anaconda3\\envs\\llmapp\\lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\durge\\anaconda3\\envs\\llmapp\\lib\\site-packages\\langchain\\embeddings\\openai.py:104\u001b[0m, in \u001b[0;36membed_with_retry.<locals>._embed_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 104\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _check_response(response)\n",
      "File \u001b[1;32mc:\\Users\\durge\\anaconda3\\envs\\llmapp\\lib\\site-packages\\openai\\api_resources\\embedding.py:33\u001b[0m, in \u001b[0;36mEmbedding.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;66;03m# This is only for the default case.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m user_provided_encoding_format:\n",
      "File \u001b[1;32mc:\\Users\\durge\\anaconda3\\envs\\llmapp\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\durge\\anaconda3\\envs\\llmapp\\lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\durge\\anaconda3\\envs\\llmapp\\lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\durge\\anaconda3\\envs\\llmapp\\lib\\site-packages\\openai\\api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    767\u001b[0m     )\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
     ]
    }
   ],
   "source": [
    "vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory='./data')\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
